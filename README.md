# Tiny-GPT "AI-да Пушкин!"
Tiny-GPT: трансформер своими лапками. 

Написала decoder-only, character-based трансформер с нуля на PyTorch на основе статьи "Attention is all you need", 2017
(https://arxiv.org/pdf/1706.03762v5.pdf)
и двух видео от Андрея Карпаты
1) https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1s

2) https://www.youtube.com/watch?v=XfpMkf4rD6E&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&index=11

Для ускорения процесса обучения использовала torch.compile() из PyTorch 2.0.
В качестве обучающего датасета был выбран роман в стихах А. С. Пушкина "Евгений Онегин" (файл в данном репозитории "input_Onegin.txt").
Для обучения использовала Т4 на Google Colab. С использованием torch.compile() процесс обучения длился менее 5ти минут.

По своей архитектуре - это только декодер, то есть отсутствует часть, выделенная красным на рисунке из оригинальной статьи "Attention is all you need"
<div>
<img src="https://github.com/lenaptv/Tiny-GPT-/blob/main/decoder-only.png" width="400" height="500"/>
</div>

Кроме того, есть также есть перестановка в отличие от архитектуры, описанной в оригинальной статье: слой нормализации применяется до самовнимания и перед feed-forward.
В результате модель генерирует некое (конечно, очень отдаленное и слабое) подобие стихов Пушкина, так как все-таки это character-based трансформер, который был обучен на небольшом датасете и при минимальных затратах временных и вычислительных ресурсов.
Но все же некоторые слова похожи на оригинал и есть некий ритм стихов Пушкина.

Код и результат генерации смотрите в ноутбуке "tiny-GPT_AI-да Пушкин.ipynb"
